{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Learning Benchmarks and Classifier models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from QueryStrategies.Random import uniform_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from QueryStrategies.Bayesian import max_entropy, bald"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from QueryStrategies.RL_based import RLAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from QueryStrategies.IRL_based import Q_Generator, Q_Sampler, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Classifier.Model import vgg, CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "import os\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from subprocess import check_output\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from collections import deque\n",
    "import seaborn as sns\n",
    "\n",
    "from skorch import NeuralNetClassifier\n",
    "from skorch import NeuralNet\n",
    "from modAL.models import ActiveLearner\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from keras.models import Model\n",
    "from keras.initializers import glorot_uniform\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Activation, Flatten\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some useful functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(y_tensor, ndims): \n",
    "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
    "    y_one_hot = torch.zeros(y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1)\n",
    "    return y_one_hot\n",
    "\n",
    "def accuracy(out, yb):\n",
    "    preds = torch.argmax(out, dim=-1)\n",
    "    return (preds == yb).float().mean()\n",
    "\n",
    "def show_image(X, y, index):\n",
    "    candidate_image = X[index]  #X_train[1000,:]\n",
    "    print(\"true label:\" + str(y[index]))\n",
    "    probs, _, _ = Classifier(X[index:index+1].float())\n",
    "    print(\"predicted label:\" + str(torch.argmax(probs, dim=-1)))\n",
    "    candidate_image = candidate_image.reshape(28,28)\n",
    "    plt.imshow(candidate_image, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "Choices of data:\n",
    "1- MNIST\n",
    "2- CIFAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST\n",
    "\n",
    "In this section, we prepare the MNIST dataset. For CIFAR, run the codes in the following section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = MNIST('.', train=True, download=True, transform=ToTensor())\n",
    "mnist_test  = MNIST('.', train=False,download=True, transform=ToTensor())\n",
    "traindataloader = DataLoader(mnist_train, shuffle=True, batch_size=60000)\n",
    "testdataloader  = DataLoader(mnist_test , shuffle=True, batch_size=10000)\n",
    "X, y = next(iter(traindataloader))\n",
    "X_test , y_test  = next(iter(testdataloader))\n",
    "\n",
    "print(\"Shape of features in training data: \", X.shape)\n",
    "print(\"Shape of labels in training data: \", y.shape)\n",
    "print(\"Shape of features in test data: \", X_test.shape)\n",
    "print(\"Shape of labels in test data: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_cross, y_train, y_cross = X[:50000], X[50000:], y[:50000], y[50000:]\n",
    "\n",
    "X_train = X_train.reshape(50000, 1, 28, 28)\n",
    "X_cross = X_cross.reshape(10000, 1, 28, 28)\n",
    "X_test = X_test.reshape(10000, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the data at the end so that we can use it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(X_train, 'Constructed_Data/X_train_MNIST.pt')\n",
    "# torch.save(y_train, 'Constructed_Data/y_train_MNIST.pt')\n",
    "# torch.save(X_cross, 'Constructed_Data/X_cross_MNIST.pt')\n",
    "# torch.save(y_cross, 'Constructed_Data/y_cross_MNIST.pt')\n",
    "# torch.save(X_test, 'Constructed_Data/X_test_MNIST.pt')\n",
    "# torch.save(y_test, 'Constructed_Data/y_test_MNIST.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR\n",
    "\n",
    "In this section, let's have alook at CIFAR dataset and prepare it for our experiments. If you use CIFAR, you have to adjust the shapes in the classifier accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CIFAR10(root='data/', download=True, transform=ToTensor())\n",
    "test_dataset = CIFAR10(root='data/', train=False, transform=ToTensor())\n",
    "classes = dataset.classes\n",
    "print(\"list of labels: \", classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_count = {}\n",
    "for _, index in dataset:\n",
    "    label = classes[index]\n",
    "    if label not in class_count:\n",
    "        class_count[label] = 0\n",
    "    class_count[label] += 1\n",
    "print(\"list of each class and its corresponding cardinality: \\n\", classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(43)\n",
    "val_size = 5000\n",
    "train_size = len(dataset) - val_size\n",
    "\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "len(train_ds), len(val_ds), len(test_dataset)\n",
    "\n",
    "batch_size=128\n",
    "\n",
    "traindataloader = DataLoader(train_ds, shuffle=True, batch_size=45000)\n",
    "valdataloader = DataLoader(val_ds, shuffle=True, batch_size=5000)\n",
    "testdataloader  = DataLoader(test_dataset , shuffle=True, batch_size=10000)\n",
    "X_train, y_train = next(iter(traindataloader))\n",
    "X_test , y_test  = next(iter(testdataloader))\n",
    "X_cross , y_cross  = next(iter(valdataloader))\n",
    "\n",
    "print(\"Shape of features in training data: \", X_train.shape)\n",
    "print(\"Shape of labels in training data: \", y_train.shape)\n",
    "print(\"Shape of features in test data: \", X_test.shape)\n",
    "print(\"Shape of labels in test data: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, _ in traindataloader:\n",
    "    print('images.shape:', images.shape)\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(make_grid(images, nrow=16).permute((1, 2, 0)))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the data at the end so that we can use it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(X_train, 'Constructed_Data/X_train_CIFAR.pt')\n",
    "# torch.save(y_train, 'Constructed_Data/y_train_CIFAR.pt')\n",
    "# torch.save(X_cross, 'Constructed_Data/X_cross_CIFAR.pt')\n",
    "# torch.save(y_cross, 'Constructed_Data/y_cross_CIFAR.pt')\n",
    "# torch.save(X_test, 'Constructed_Data/X_test_CIFAR.pt')\n",
    "# torch.save(y_test, 'Constructed_Data/y_test_CIFAR.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function for the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42 #For more repetitive results\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "#...... Set the budget in each episode and the number of episodes\n",
    "budget = 500\n",
    "num_episodes = 1\n",
    "\n",
    "#...... Params for Classifier\n",
    "output_shape_of_penultimate_layer = 128\n",
    "num_classes = 10\n",
    "lr_Classifier = 0.001\n",
    "\n",
    "Classifier = CNN(output_shape_of_penultimate_layer, num_classes)\n",
    "Classifier = Classifier.float()\n",
    "optimizer_Classifier = optim.Adam(Classifier.parameters(), lr=lr_Classifier)\n",
    "criterion = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_Classifier(model, features, labels, optimizer, criterion=F.cross_entropy):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    probs, logits, penult_out = model(features.float())\n",
    "        \n",
    "    loss_func = criterion    \n",
    "    loss = loss_func(logits, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a list of indices for initial rounds of play\n",
    "initial_idx = np.array([],dtype=np.int)\n",
    "for i in range(10):\n",
    "    idx = np.random.choice(np.where(y_train==i)[0], size=2, replace=False)\n",
    "    initial_idx = np.concatenate((initial_idx, idx))\n",
    "\n",
    "X_initial = X_train[initial_idx]\n",
    "y_initial = y_train[initial_idx]\n",
    "\n",
    "X_pool = np.delete(X_train, initial_idx, axis=0)\n",
    "y_pool = np.delete(y_train, initial_idx, axis=0)\n",
    "\n",
    "print(initial_idx)\n",
    "print(y_train[initial_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the data\n",
    "# torch.save(X_initial, 'Constructed_Data/X_initial.pt')\n",
    "# torch.save(y_initial, 'Constructed_Data/y_initial.pt')\n",
    "# torch.save(X_pool, 'Constructed_Data/X_pool.pt')\n",
    "# torch.save(y_pool, 'Constructed_Data/y_pool.pt')\n",
    "# np.save('Constructed_Data/initial_idx', np.array(initial_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active Learning Procedure\n",
    "\n",
    "It receives the query strategy, e.g., RL-based strategy, as the input and performs the active learning task using the given strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_learning_procedure(query_strategy,\n",
    "                              X_cross,\n",
    "                              y_cross,\n",
    "                              X_pool,\n",
    "                              y_pool,\n",
    "                              X_initial,\n",
    "                              y_initial,\n",
    "                              Classifier,\n",
    "                              optimizer_Classifier,\n",
    "                              criterion=F.cross_entropy,\n",
    "                              n_queries=num_episodes*budget, #100\n",
    "                              n_instances=1): #10\n",
    "    \n",
    "    loss = train_Classifier(Classifier, X_initial, y_initial, optimizer_Classifier, criterion)\n",
    "    \n",
    "    probs, _, _ = Classifier(X_cross.float())\n",
    "    model_accuracy = accuracy(probs, y_cross)\n",
    "    perf_hist = [model_accuracy]\n",
    "    \n",
    "    X_lab = X_initial\n",
    "    y_lab = y_initial\n",
    "    for index in range(n_queries):\n",
    "        query_idx, query_instance = query_strategy(Classifier, X_pool, n_instances)\n",
    "\n",
    "        X_lab = np.append(X_lab, X_pool[query_idx], axis=0)\n",
    "        y_lab = np.append(y_lab, y_pool[query_idx], axis=0)\n",
    "#         print(X_lab.shape)\n",
    "#         print(y_lab.shape)\n",
    "#         print(type(torch.from_numpy(X_lab)))\n",
    "#         print(torch.from_numpy(X_lab).shape)\n",
    "        \n",
    "        loss = train_Classifier(Classifier, torch.from_numpy(X_lab), torch.from_numpy(y_lab), optimizer_Classifier, criterion) \n",
    "#         loss = train_Classifier(Classifier, X_pool[query_idx], y_pool[query_idx], optimizer_Classifier, criterion) #learner.teach(X_pool[query_idx], y_pool[query_idx])\n",
    "\n",
    "        X_pool = np.delete(X_pool, query_idx, axis=0)\n",
    "        y_pool = np.delete(y_pool, query_idx, axis=0)\n",
    "        \n",
    "        probs, _, _ = Classifier(X_cross.float())\n",
    "        model_accuracy = accuracy(probs, y_cross)\n",
    "        \n",
    "        print('Accuracy after query {n}: {acc:0.4f}'.format(n=index + 1, acc=model_accuracy))\n",
    "        perf_hist.append(model_accuracy)\n",
    "    return perf_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the benchmarks\n",
    "\n",
    "We use different benchmark algorithms to perform active learning. To this end, they actively investigate each given unlablled data point and decide to ask for the corresponding label or not. The labelling budget is limited, which means that algorithms can ask for ground truth labels only for a certain number of data points.\n",
    "\n",
    "In each case, we record the accuracy of the classifier over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "entropy_perf_hist = active_learning_procedure(max_entropy,\n",
    "                                              X_cross,\n",
    "                                              y_cross,\n",
    "                                              X_pool,\n",
    "                                              y_pool,\n",
    "                                              X_initial,\n",
    "                                              y_initial,\n",
    "                                              Classifier,\n",
    "                                              optimizer_Classifier,\n",
    "                                              criterion,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('Results_Data/entropy_perf_hist', np.array(entropy_perf_hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "bald_perf_hist = active_learning_procedure(bald,\n",
    "                                           X_cross,\n",
    "                                           y_cross,\n",
    "                                           X_pool,\n",
    "                                           y_pool,\n",
    "                                           X_initial,\n",
    "                                           y_initial,\n",
    "                                           Classifier,\n",
    "                                           optimizer_Classifier,\n",
    "                                           criterion,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('Results_Data/bald_perf_hist', np.array(bald_perf_hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "uniform_perf_hist = active_learning_procedure(uniform_random,\n",
    "                                              X_cross,\n",
    "                                              y_cross,\n",
    "                                              X_pool,\n",
    "                                              y_pool,\n",
    "                                              X_initial,\n",
    "                                              y_initial,\n",
    "                                              Classifier,\n",
    "                                              optimizer_Classifier,\n",
    "                                              criterion,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('Results_Data/uniform_perf_hist', np.array(uniform_perf_hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#...... Params for RL_based\n",
    "epsilon = 0.1\n",
    "EPSILON_DECAY = 0.1\n",
    "MIN_EPSILON = 0.001\n",
    "num_actions = 2\n",
    "lr_RL = 1e-3\n",
    "state_dim = output_shape_of_penultimate_layer + num_classes #state_shape[0], state_shape = (output_shape_of_penultimate_layer + num_classes, )\n",
    "Q_hid_dim_1 = 256 #8\n",
    "Q_hid_dim_2 = 128 #4\n",
    "REPLAY_MEMORY_SIZE = 64 #To create an array with last \"REPLAY_MEMORY_SIZE\" steps for training\n",
    "MIN_REPLAY_MEMORY_SIZE = 16 #Start training only if certain number of samples is already saved\n",
    "MINIBATCH_SIZE = 64 #Not Used -- Batch from replay_memory\n",
    "BATCH_SIZE = 32\n",
    "UPDATE_TARGET_EVERY = 0 #To update target network. Target network is used to predict future_qs_list, model_RL is used to predict current_qs_list\n",
    "DISCOUNT = 0.1\n",
    "\n",
    "RL_Model = RLAgent(state_dim, num_actions, Q_hid_dim_1, Q_hid_dim_2, \n",
    "                   REPLAY_MEMORY_SIZE, MIN_REPLAY_MEMORY_SIZE, BATCH_SIZE, MINIBATCH_SIZE, DISCOUNT, UPDATE_TARGET_EVERY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For RL-based approach, we need to re-define the active learning function as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AL With Function\n",
    "def active_learning_procedure_RL(RL_Model,\n",
    "                              X_cross,\n",
    "                              y_cross,\n",
    "                              X_pool,\n",
    "                              y_pool,\n",
    "                              X_initial,\n",
    "                              y_initial,\n",
    "                              Classifier,\n",
    "                              optimizer_Classifier,\n",
    "                              n_episodes=num_episodes, \n",
    "                              epsilon=epsilon,\n",
    "                              MIN_EPSILON=MIN_EPSILON,\n",
    "                              EPSILON_DECAY=EPSILON_DECAY,\n",
    "                              penult_len=output_shape_of_penultimate_layer,\n",
    "                              num_classes=10,\n",
    "                              num_actions=2,\n",
    "                              criterion=F.cross_entropy,\n",
    "                              n_queries=budget, #100\n",
    "                              n_instances=1, #10\n",
    "                              random_selection=False): \n",
    "\n",
    "    loss = train_Classifier(Classifier, X_initial, y_initial, optimizer_Classifier, criterion)\n",
    "    \n",
    "    probs, _, _ = Classifier(X_cross.float())\n",
    "    model_accuracy = accuracy(probs, y_cross)\n",
    "    perf_hist = [model_accuracy]\n",
    "    \n",
    "    rew = []\n",
    "    saver_count = False\n",
    "    X_lab = X_initial\n",
    "    y_lab = y_initial\n",
    "    t = -1\n",
    "    for episode in tqdm(range(0, int(n_episodes)), ascii=True, unit='episodes'):\n",
    "        step = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            \n",
    "            t = t+1\n",
    "            print(t)\n",
    "            idx = np.random.choice(range(len(X_pool)), size=n_instances, replace=False)\n",
    "\n",
    "            #...... Receive the Current state\n",
    "            probs, _, penult_out = Classifier(X_pool[idx].float())\n",
    "            penult_out = penult_out.view(penult_len)\n",
    "            probs = probs.view(num_classes)\n",
    "            state_at_t = torch.cat((penult_out, probs), -1)\n",
    "            \n",
    "            probs, _, _ = Classifier(X_cross.float())\n",
    "            model_accuracy_before = accuracy(probs, y_cross)\n",
    "    \n",
    "            #RL picks an action to label/not label #query_idx, query_instance = query_strategy(Classifier, X_pool, n_instances)\n",
    "            Q_values = RL_Model.get_qs(state_at_t.detach().numpy())\n",
    "            action_at_t = np.argmax(Q_values) # Get action from Q table\n",
    "\n",
    "            if random_selection:\n",
    "                if np.random.random() > epsilon:\n",
    "                    action_at_t = action_at_t\n",
    "                else: # Get random action\n",
    "                    action_at_t = np.random.randint(0, num_actions) # Get random action: 2 possible choices: i) 1 == label ii) 0 == do not label\n",
    "        \n",
    "            if action_at_t == 1:\n",
    "                step = step + 1\n",
    "                \n",
    "                X_lab = np.append(X_lab, X_pool[idx], axis=0)\n",
    "                y_lab = np.append(y_lab, y_pool[idx], axis=0)\n",
    "\n",
    "                loss = train_Classifier(Classifier, torch.from_numpy(X_lab), torch.from_numpy(y_lab), optimizer_Classifier, criterion) \n",
    "                \n",
    "#                 X_pool = np.delete(X_pool, idx, axis=0)\n",
    "#                 y_pool = np.delete(y_pool, idx, axis=0)\n",
    "\n",
    "                probs, _, _ = Classifier(X_cross.float())\n",
    "                model_accuracy_after = accuracy(probs, y_cross)\n",
    "                \n",
    "                print('Accuracy after query {n}: {acc:0.4f}'.format(n=step, acc=model_accuracy_after))\n",
    "                perf_hist.append(model_accuracy_after)\n",
    "        \n",
    "            else:\n",
    "                model_accuracy_after = model_accuracy_before\n",
    "                \n",
    "            reward_at_t = model_accuracy_after - model_accuracy_before #Compute reward\n",
    "            rew.append(reward_at_t)\n",
    "\n",
    "            #...... Receive the New state\n",
    "            probs, _, penult_out = Classifier(X_pool[idx].float())\n",
    "            penult_out = penult_out.view(penult_len)\n",
    "            probs = probs.view(num_classes)\n",
    "            state_at_next_t = torch.cat((penult_out, probs), -1)\n",
    "\n",
    "            if step >= budget: #Update the step and check episode using budget\n",
    "                done = True\n",
    "\n",
    "            replay_memory_DataColl = RL_Model.update_replay_memory_DataColl((state_at_t.detach().numpy(), action_at_t)) #Add Later: , log_q_tau_var_D, 1\n",
    "\n",
    "            if episode == n_episodes-1 and step == budget:\n",
    "                print(\"episode before saving:\" + str(episode))\n",
    "                print(\"step before saving:\" + str(step))\n",
    "                saver_count = True\n",
    "\n",
    "            #...... Updating the RL model (training the weights of RL model),  Every step we update replay memory and train main network\n",
    "            RL_Model.update_replay_memory((state_at_t.detach().numpy(), action_at_t, reward_at_t.detach().numpy(), state_at_next_t.detach().numpy(), done))\n",
    "            RL_Model.train(done, step, saver_count) #train(self, terminal_state, step, saver_count)\n",
    "\n",
    "            X_pool = np.delete(X_pool, idx, axis=0)\n",
    "            y_pool = np.delete(y_pool, idx, axis=0)\n",
    "            \n",
    "        if random_selection: #Decay epsilon\n",
    "            if epsilon > MIN_EPSILON:\n",
    "                epsilon *= EPSILON_DECAY\n",
    "                epsilon = max(MIN_EPSILON, epsilon)\n",
    "\n",
    "    return perf_hist, replay_memory_DataColl, rew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "RL_perf_hist, RL_replay_memory_DataColl, RL_rew = active_learning_procedure_RL(RL_Model,\n",
    "                                              X_cross,\n",
    "                                              y_cross,\n",
    "                                              X_pool,\n",
    "                                              y_pool,\n",
    "                                              X_initial,\n",
    "                                              y_initial,\n",
    "                                              Classifier,\n",
    "                                              optimizer_Classifier,\n",
    "                                              n_episodes=num_episodes, \n",
    "                                              epsilon=epsilon,\n",
    "                                              MIN_EPSILON=MIN_EPSILON,\n",
    "                                              EPSILON_DECAY=EPSILON_DECAY,\n",
    "                                              penult_len=output_shape_of_penultimate_layer,\n",
    "                                              num_classes=10,\n",
    "                                              num_actions=2,\n",
    "                                              criterion=F.cross_entropy,\n",
    "                                              n_queries=budget, #100\n",
    "                                              n_instances=1, #10\n",
    "                                              random_selection=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IRL_perf_hist = np.load('Results_Data/perf_hist_IRL_4.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the Results\n",
    "\n",
    "We compare the accuracy trend of the classifier over time for different query strategies as new labelled data points are fed to update the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set()\n",
    "# sns.set_style(\"white\")\n",
    "fig = plt.figure(figsize=[8, 5])\n",
    "plt.plot(np.arange(120)[::20], 100*entropy_perf_hist[::20], color = 'blue', marker = \"d\", markersize = 5, fillstyle='none', markeredgewidth=1.5, label=\"Max Entropy\")\n",
    "plt.plot(np.arange(120)[::20], 100*uniform_perf_hist[:120][::20], color = 'black', marker = \"s\", markersize = 5, fillstyle='none', markeredgewidth=1.5, label=\"Random\")\n",
    "# plt.plot(np.arange(120)[::20], 100*perf_hist_IRL[::20], color = 'orange', marker = \"o\", markersize = 5, fillstyle='none', markeredgewidth=1.5, label=\"Our Method\")\n",
    "plt.plot(np.arange(120)[::20], 100*RL_perf_hist[::20], color = 'green', marker = \">\", markersize = 5, fillstyle='none', markeredgewidth=1.5, label=\"RL\")\n",
    "\n",
    "plt.legend(loc = 'lower right', fontsize = 20)\n",
    "# plt.title(f\"Accuracy vs. Num Labelled data\")\n",
    "plt.xlabel(\"#Labelled Data\", fontsize = 22)\n",
    "plt.ylabel(\"Accuracy\", fontsize = 22)\n",
    "plt.xticks(fontsize = 15)\n",
    "plt.yticks(fontsize = 15)\n",
    "# plt.grid()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig.savefig(\"Results_Before_Transfer_to_Test_Phase.png\", format = 'png', dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
